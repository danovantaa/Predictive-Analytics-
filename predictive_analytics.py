# -*- coding: utf-8 -*-
"""Predictive_Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qx9umPmKLpTS5OkKYHj9g4dw6sZt43t2

# Laporan Proyek Machine Learning - Zefanya Danovanta Tarigan

## Domain Proyek
Domain yang dipilih untuk proyek *machine learning* ini adalah **Kesehatan**, dengan judul **Predictive Analytics : Deteksi Balita Stunting**
- **Latar Belakang :**  
Stunting adalah kondisi terhambatnya pertumbuhan pada anak balita akibat kurang gizi kronis sehingga anak terlibat lebih pendek dari pertumbuhan usianya. Hal tersebut akan berdampak pada perkembangan anak, maka pemantauan pertumbuhan dan perkembang balita sangat penting dilakukan untuk mengetahui hambatan pertumbuhan (growth faltering) sejak dini.
Deteksi dini menjadi salah satu langkah paling penting dalam menangani stunting. Dengan mendeteksi stunting lebih awal, intervensi gizi dan perawatan medis dapat diberikan secara tepat waktu, sehingga risiko jangka panjang dapat diminimalkan. Namun, dalam praktiknya, deteksi stunting sering kali masih bergantung pada metode tradisional yang membutuhkan waktu lebih lama, dan kadang kurang efektif dalam menjangkau populasi yang luas.

## Business Understanding

### Problem Statements
Berdasarkan latar belakang di atas, berikut ini merupakan rincian masalah yang dapat diselesaikan pada proyek ini :
-
-

### Goals
Tujuan dari proyek ini adalah:
-
-

## Data Understanding
Dataset yang digunakan dalam proyek ini adalah kumpulan data berdasarkan rumus z-score penentuan stunting menurut WHO (World Health Organization), yang berfokus pada deteksi stunting pada balita. Dataset ini terdiri dari 121000 baris dan 4 kolom, diataranya :  
1. `Umur (Bulan)` : Mengindikasikan usia balita dalam bulan. Rentang usia ini penting untuk menentukan fase pertumbuhan anak dan membandingkannya dengan standar pertumbuhan yang sehat. (Umur 0 sampai 60 bulan)
2. `Jenis Kelamin` : Terdapat dua kategori dalam kolom ini, **laki-laki** dan **perempuan**.
3. `Tinggi Badan` :Dicatat dalam centimeter, tinggi badan adalah indikator utama untuk menilai pertumbuhan fisik balita.
4. `Status Gizi` : Kolom ini dikategorikan menjadi 4 status - 'severely stunting', 'stunting', 'normal', dan 'tinggi'. 'Severely stunting' menunjukkan kondisi sangat serius (<-3 SD), 'stunting' menunjukkan kondisi stunting (-3 SD sd <-2 SD), 'normal' mengindikasikan status gizi yang sehat (-2 SD sd +3 SD), dan 'tinggi' (height) menunjukkan pertumbuhan di atas rata-rata (>+3 SD)

## Data Preparation
Teknik yang digunakan dalam penyiapan data (Data Preparation) yaitu:
-
-

### Import Library
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from google.colab import files
import os
import zipfile

"""### Menyiapkan Kredensial Kaggle
Dataset yang akan dipakai dalam proyek ini diambil dari platform Kaggle. Maka dari itu, sebelum dapat mengunduh data, harus mengunggah kredensial berupa file JSON yang dapat di-generate melalui profil akun Kaggle.
"""

# Upload kaggle.json
uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}"'.format(
      name=fn))

# Ubah permission file
!chmod 600 /content/kaggle.json

# Setup Kaggle environment
os.environ['KAGGLE_CONFIG_DIR'] = "/content"

# Download dataset
!kaggle datasets download -d rendiputra/stunting-balita-detection-121k-rows

# melakukan ekstraksi pada file zip
local_zip = 'stunting-balita-detection-121k-rows.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content/stunting-balita-detection-121k-rows/')
zip_ref.close()

# Menghapus berkas zip yang sudah tidak diperlukan
!rm stunting-balita-detection-121k-rows.zip

"""### Data Understanding
Langkah pertama untuk memahami data yaitu dengan melihat isi dari direktori dataset yang telah diunduh


"""

# Cek isi direktori dataset
os.listdir('/content/stunting-balita-detection-121k-rows')

"""Dari keluaran di atas, dapat diketahui bahwa ada dua berkas data, diantaranya adalah dataset utama dan berkas lainnya adalah merupakan dokumentasi.

Untuk Melihat data dokumentasinya dengan menggunakan *pandas*
"""

# Cek dokumentasi dataset
data = pd.read_csv('/content/stunting-balita-detection-121k-rows/data_balita.csv')
data.head()

"""Dari dataframe di atas kita dapat melihat bahwa pada dataset ini terdapat 4 kolom. Diantaranya:
1. `Umur (Bulan)` : Mengindikasikan usia balita dalam bulan. Rentang usia ini penting untuk menentukan fase pertumbuhan anak dan membandingkannya dengan standar pertumbuhan yang sehat. (Umur 0 sampai 60 bulan)
2. `Jenis Kelamin` : Terdapat dua kategori dalam kolom ini, **laki-laki** dan **perempuan**.
3. `Tinggi Badan` :Dicatat dalam centimeter, tinggi badan adalah indikator utama untuk menilai pertumbuhan fisik balita.
4. `Status Gizi` : Kolom ini dikategorikan menjadi 4 status - 'severely stunting', 'stunting', 'normal', dan 'tinggi'. 'Severely stunting' menunjukkan kondisi sangat serius (<-3 SD), 'stunting' menunjukkan kondisi stunting (-3 SD sd <-2 SD), 'normal' mengindikasikan status gizi yang sehat (-2 SD sd +3 SD), dan 'tinggi' (height) menunjukkan pertumbuhan di atas rata-rata (>+3 SD)
"""

data.shape

data.describe()

data.info()
print("Jumlah Duplikasi : ", data.duplicated().sum())

"""Dari Output diatas, kita dapat mengetahui Banyak nilai yang ada pada masing masing kolom beserta tipe data kolom tersebut, lalu terdapat juga nilai yang sama (mengalami duplikasi) sebanyak 81574"""

data.isnull().sum()

"""Dari output diatas, dapat diambil kesimpulan bahwa masing masing kolom tidak mempuyai nilai yang kosong (Missing Value)"""

data = data.drop_duplicates()

"""### Visualisasi Data dan analisis eksplorasi data (EDA)

"""

numerical_columns = data.select_dtypes(include=["int64","float64"]).columns
category_columns = data.select_dtypes(include= ["object"]).columns

data[numerical_columns].hist(bins=10)
plt.tight_layout()
plt.show()

plt.figure()
correlation_matrix = data[numerical_columns].corr()
sns.heatmap(correlation_matrix, annot= True, fmt='.2f', cmap='coolwarm', square=True)
plt.title("Matrix Korelasi", fontsize=14)
plt.show()

for col in category_columns:
    plt.figure()
    data[col].value_counts().plot.pie(
        autopct='%1.1f%%',
        startangle=90,
        explode=[0.05] * len(data[col].value_counts()),
        shadow=True,
    )
    plt.title(f"Distribution of {col}", fontsize=18)
    plt.ylabel('')
    plt.tight_layout()
    plt.show()

for col in numerical_columns:
    sns.boxplot(x=data[col])
    plt.title(f"Boxplot of {col}" )
    plt.show()

"""### Data Preparation

#### Menangani Duplikasi
Data tersebut mempunyai nilai duplikasi sebanyak 81574, maka dari itu perlu menghapus nilai nilai duplikat tersebut dengan cara `.drop_duplicates()`
"""

data = data.drop_duplicates()

"""Untuk memastikan apakah data tersebut sudah tidak ada lagi duplikasi, maka kita perlu mengecek kembali dengan cara `.duplicated().sum()`"""

data.info()
print("Jumlah Duplikasi : ", data.duplicated().sum())

data.shape

"""#### Label Encoder
Melakukan Encoding menggunakan Label Encode dari library sklearn.preprocessing pada kolom `Status Gizi` lalu di pindahkan ke dalam kolom `Gizi_Encode` dan kolom sebelumya akan dihapus
"""

label_encoder = LabelEncoder()
data["Gizi_Encode"] = label_encoder.fit_transform(data["Status Gizi"])

data = data.drop("Status Gizi", axis=1)

"""#### One-Hot Encoding
Melakukan One-Hot Encoding pada variabel bertipe data category yang tersisa yaitu `Jenis Kelamin`
"""

data = pd.get_dummies(data)
data.head()

data.info()

"""#### Split Data
Membagi dataset menjadi data latih dan data uji menggunakan bantuan **train_test_split**. Pembagian dataset ini bertujuan agar nantinya dapat digunakan untuk melatih dan mengevaluasi kinerja model. Pada proyek ini, 80% dataset digunakan untuk melatih model, dan 20% sisanya digunakan untuk mengevaluasi model.
"""

X = data.drop(["Gizi_Encode"], axis=1)
y = data["Gizi_Encode"]

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)

# Memberikan Informasi dari banyak data yang telah dibagi
print(f'Total of sample in whole dataset: {len(X)}')
print(f'Total of sample in train dataset: {len(X_train)}')
print(f'Total of sample in test dataset: {len(X_test)}')

"""#### Normalisasi Data
Menggunakan **MinMaxScaler**, yaitu teknik normalisasi yang mentransformasikan nilai fitur atau variabel ke dalam rentang [0,1] yang berarti bahwa nilai minimum dan maksimum dari fitur/variabel masing-masing adalah 0 dan 1
"""

scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""### Modeling
Pada tahap modeling ini dibuat beberapa model dengan algoritma yang berbeda-beda. Pada proyek ini akan dibuat 4 model, diantaranya yaitu menggunakan KNN, Random Forest, SVM, dan Naive Bayes
"""

models = pd.DataFrame(index=['accuracy_score'],
                      columns=['KNN', 'RandomForest', 'SVM', 'Naive Bayes'])

"""#### KNN (K-Nearest Neighbor)"""

knn = KNeighborsClassifier()

# Mencoba nilai n_neighbors dari 1 hingga 20
param_grid = {'n_neighbors': range(2, 21)}
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Hasil terbaik
print("Best n_neighbors:", grid_search.best_params_['n_neighbors'])
print("Best accuracy on training set:", grid_search.best_score_)

best_knn = grid_search.best_estimator_
knn_pred = best_knn.predict(X_test)

models.loc['accuracy_score','KNN'] = accuracy_score(y_test, knn_pred)

"""#### Random Forest"""

# Buat model prediksi dengan Random Forest
model_rf = RandomForestClassifier(random_state = 42)
model_rf.fit(X_train, y_train)

# Lakukan prediksi dengan model Random Forest
rf_pred = model_rf.predict(X_test)

# Hitung metriks akurasi dan simpan hasilnya
models.loc['accuracy_score','RandomForest'] = accuracy_score(y_test, rf_pred)

"""#### Support Vector Classifier"""

# Buat model prediksi dengan Support Vector Machine Classifier
model_svc = SVC()
model_svc.fit(X_train, y_train)

# Lakukan prediksi dengan model SVM Classifier
svc_pred = model_svc.predict(X_test)

# Hitung metriks akurasi dan simpan hasilnya
models.loc['accuracy_score','SVM'] = accuracy_score(y_test, svc_pred)

"""#### Naive Bayes"""

# Buat model prediksi dengan Bernoulli Naive Bayes
model_nb = BernoulliNB()
model_nb.fit(X_train, y_train)

# Lakukan prediksi dengan model Naive Bayes
nb_pred = model_nb.predict(X_test)

# Hitung metriks akurasi dan simpan hasilnya
models.loc['accuracy_score','Naive Bayes'] = accuracy_score(y_test, nb_pred)

"""### Evaluation
Setelah mendapatkan beberapa model, maka dapat dibandingkan akurasi prediksinya untuk mendapatkan model dengan kinerja yang terbaik. Agar lebih mudah dapat menggunakan visualisasi seperti berikut.
"""

# Menampilkan perbandingan akurasi beberapa model yang telah dibuat
plt.figure(figsize=(10, 6))
plt.bar(models.columns, models.loc['accuracy_score'].astype(float))
plt.title("Perbandingan Akurasi Model")
plt.xlabel('Model')
plt.ylabel('Akurasi')
plt.ylim(0, 1)
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()

models

best_model = models.loc['accuracy_score'].astype(float).idxmax()
print(f"Model terbaik adalah {best_model} dengan akurasi {models.loc['accuracy_score', best_model]}")